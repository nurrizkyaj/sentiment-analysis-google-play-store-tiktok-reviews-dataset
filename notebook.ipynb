{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1282f92a",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e164643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nurri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nurri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Bidirectional, LSTM, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595edd6",
   "metadata": {},
   "source": [
    "# **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b6af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "app_reviews_df = pd.read_csv('reviews_tiktok.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b5a38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Review",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b4fa9358-3550-4a99-9fe4-414478e466bd",
       "rows": [
        [
         "0",
         "untuk apk ini Bagusss banget!! aku suka, tapi udh 4 hari ini tik tok ku bermasalah kayak setiap pencet logonya itu buat masuk malah keluar, padahal wifi bagus, memori belum penuh sama sekali tapi tetep nggak bisa, terus tiba-tiba ada notifikasi kayak ada bug..ini gimana ya? semoga bug nya hilang soalnya tanpa tik tok saya bosen :v"
        ],
        [
         "1",
         "tiktok sekarang aplikasi nya berat bangetü•≤ü•≤... padahal sering dibersihkan tapi tetap aja berat, berat nya tuh bukan yang menuhin memori ya, tapi aplikasinya tu sangking beratnya jadi patah\" parah pake banget asli, trus kalo baru buka tiktok tuh makin parah... plus loading nya pas baru di buka lemot, padahal dulu pas di hp yang penyimpanan cuman 64gb ram 2 itu lancar\" aja, sekarang hp yang lebih mantap ketimbang yang lama malah patah\" parah dan lemotü•≤ü•≤.. tolong dong segera perbaiki patah\" nya"
        ],
        [
         "2",
         "awalnya aku kira hpku yg rusak soalnya setiap bukan tiktok hp ku kadang kadang mati dan lemot, masuk tiktok yah lama banget sampai berjam-jam perasaan di aplikasi lainnya nggak kayak gitu / dan ternyata tiktok nya yg bermasalah , jadi tolong di perbaiki bug nya, semoga di baca üò¢üôèüèª"
        ],
        [
         "3",
         "tiktok, sekarang banyak updatean tapi malah tambah ngelag, contohnya kalau mau masuk keluar sendiri, kadang juga lagi di pencarian keluar ke tulus pencarian, kadang juga kalau lagi nonton di beranda sering keluar, sekarang juga patah patah videonya (sering)"
        ],
        [
         "4",
         "sebenarnya aplikasi nya sudah bagus, hanya saja kenapa tiktok saya tidak ada fitur foto nya? dan filter nya juga hilang, kuharap tiktok segera memperbaiki nya, kukira sih tiktok ku yg rusak eh ternyata pas di download ulang juga tetap sama. mohon kerjasamanya ya, tiktok."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>untuk apk ini Bagusss banget!! aku suka, tapi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiktok sekarang aplikasi nya berat bangetü•≤ü•≤......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awalnya aku kira hpku yg rusak soalnya setiap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tiktok, sekarang banyak updatean tapi malah ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sebenarnya aplikasi nya sudah bagus, hanya saj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  untuk apk ini Bagusss banget!! aku suka, tapi ...\n",
       "1  tiktok sekarang aplikasi nya berat bangetü•≤ü•≤......\n",
       "2  awalnya aku kira hpku yg rusak soalnya setiap ...\n",
       "3  tiktok, sekarang banyak updatean tapi malah ta...\n",
       "4  sebenarnya aplikasi nya sudah bagus, hanya saj..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the DataFrame\n",
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26de088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58500 entries, 0 to 58499\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  58500 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 457.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show summary information of the raw reviews DataFrame\n",
    "app_reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52171a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate\n",
    "clean_df = app_reviews_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7039a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58439 entries, 0 to 58499\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  58439 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 913.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show summary information of the cleaned DataFrame\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a807893",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bbfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def cleaning_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'RT[\\s]', '', text)\n",
    "    text = re.sub(r\"http\\S+\", '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def casefolding_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\", \"plisss\": \"tolong\", \"bgttt\": \"banget\", \"indo\": \"indonesia\", \"bgtt\": \"banget\", \"ad\": \"ada\", \"rv\": \"redvelvet\", \"plis\": \"tolong\", \"pls\": \"tolong\", \"cr\": \"sumber\", \"cod\": \"bayar ditempat\", \"adlh\": \"adalah\", \"afaik\": \"as far as i know\", \"ahaha\": \"haha\", \"aj\": \"saja\", \"ajep-ajep\": \"dunia gemerlap\", \"ak\": \"saya\", \"akika\": \"aku\", \"akkoh\": \"aku\", \"akuwh\": \"aku\", \"alay\": \"norak\", \"alow\": \"halo\", \"ambilin\": \"ambilkan\", \"ancur\": \"hancur\", \"anjrit\": \"anjing\", \"anter\": \"antar\", \"ap2\": \"apa-apa\", \"apasih\": \"apa sih\", \"apes\": \"sial\", \"aps\": \"apa\", \"aq\": \"saya\", \"aquwh\": \"aku\", \"asbun\": \"asal bunyi\", \"aseekk\": \"asyik\", \"asekk\": \"asyik\", \"asem\": \"asam\", \"aspal\": \"asli tetapi palsu\", \"astul\": \"asal tulis\", \"ato\": \"atau\", \"au ah\": \"tidak mau tahu\", \"awak\": \"saya\", \"ay\": \"sayang\", \"ayank\": \"sayang\", \"b4\": \"sebelum\", \"bakalan\": \"akan\", \"bandes\": \"bantuan desa\", \"bangedh\": \"banget\", \"banpol\": \"bantuan polisi\", \"banpur\": \"bantuan tempur\", \"basbang\": \"basi\", \"bcanda\": \"bercanda\", \"bdg\": \"bandung\", \"begajulan\": \"nakal\", \"beliin\": \"belikan\", \"bencong\": \"banci\", \"bentar\": \"sebentar\", \"ber3\": \"bertiga\", \"beresin\": \"membereskan\", \"bete\": \"bosan\", \"beud\": \"banget\", \"bg\": \"abang\", \"bgmn\": \"bagaimana\", \"bgt\": \"banget\", \"bijimane\": \"bagaimana\", \"bintal\": \"bimbingan mental\", \"bkl\": \"akan\", \"bknnya\": \"bukannya\", \"blegug\": \"bodoh\", \"blh\": \"boleh\", \"bln\": \"bulan\", \"blum\": \"belum\", \"bnci\": \"benci\", \"bnran\": \"yang benar\", \"bodor\": \"lucu\", \"bokap\": \"ayah\", \"boker\": \"buang air besar\", \"bokis\": \"bohong\", \"boljug\": \"boleh juga\", \"bonek\": \"bocah nekat\", \"boyeh\": \"boleh\", \"br\": \"baru\", \"brg\": \"bareng\", \"bro\": \"saudara laki-laki\", \"bru\": \"baru\", \"bs\": \"bisa\", \"bsen\": \"bosan\", \"bt\": \"buat\", \"btw\": \"ngomong-ngomong\", \"buaya\": \"tidak setia\", \"bubbu\": \"tidur\", \"bubu\": \"tidur\", \"bumil\": \"ibu hamil\", \"bw\": \"bawa\", \"bwt\": \"buat\", \"byk\": \"banyak\", \"byrin\": \"bayarkan\", \"cabal\": \"sabar\", \"cadas\": \"keren\", \"calo\": \"makelar\", \"can\": \"belum\", \"capcus\": \"pergi\", \"caper\": \"cari perhatian\", \"ce\": \"cewek\", \"cekal\": \"cegah tangkal\", \"cemen\": \"penakut\", \"cengengesan\": \"tertawa\", \"cepet\": \"cepat\", \"cew\": \"cewek\", \"chuyunk\": \"sayang\", \"cimeng\": \"ganja\", \"cipika cipiki\": \"cium pipi kanan cium pipi kiri\", \"ciyh\": \"sih\", \"ckepp\": \"cakep\", \"ckp\": \"cakep\", \"cmiiw\": \"correct me if i'm wrong\", \"cmpur\": \"campur\", \"cong\": \"banci\", \"conlok\": \"cinta lokasi\", \"cowwyy\": \"maaf\", \"cp\": \"siapa\", \"cpe\": \"capek\", \"cppe\": \"capek\", \"cucok\": \"cocok\", \"cuex\": \"cuek\", \"cumi\": \"Cuma miscall\", \"cups\": \"culun\", \"curanmor\": \"pencurian kendaraan bermotor\", \"curcol\": \"curahan hati colongan\", \"cwek\": \"cewek\", \"cyin\": \"cinta\", \"d\": \"di\", \"dah\": \"deh\", \"dapet\": \"dapat\", \"de\": \"adik\", \"dek\": \"adik\", \"demen\": \"suka\", \"deyh\": \"deh\", \"dgn\": \"dengan\", \"diancurin\": \"dihancurkan\", \"dimaafin\": \"dimaafkan\", \"dimintak\": \"diminta\", \"disono\": \"di sana\", \"dket\": \"dekat\", \"dkk\": \"dan kawan-kawan\", \"dll\": \"dan lain-lain\", \"dlu\": \"dulu\", \"dngn\": \"dengan\", \"dodol\": \"bodoh\", \"doku\": \"uang\", \"dongs\": \"dong\", \"dpt\": \"dapat\", \"dri\": \"dari\", \"drmn\": \"darimana\", \"drtd\": \"dari tadi\", \"dst\": \"dan seterusnya\", \"dtg\": \"datang\", \"duh\": \"aduh\", \"duren\": \"durian\", \"ed\": \"edisi\", \"egp\": \"emang gue pikirin\", \"eke\": \"aku\", \"elu\": \"kamu\", \"emangnya\": \"memangnya\", \"emng\": \"memang\", \"endak\": \"tidak\", \"enggak\": \"tidak\", \"envy\": \"iri\", \"ex\": \"mantan\", \"fax\": \"facsimile\", \"fifo\": \"first in first out\", \"folbek\": \"follow back\", \"fyi\": \"sebagai informasi\", \"gaada\": \"tidak ada uang\", \"gag\": \"tidak\", \"gaje\": \"tidak jelas\", \"gak papa\": \"tidak apa-apa\", \"gan\": \"juragan\", \"gaptek\": \"gagap teknologi\", \"gatek\": \"gagap teknologi\", \"gawe\": \"kerja\", \"gbs\": \"tidak bisa\", \"gebetan\": \"orang yang disuka\", \"geje\": \"tidak jelas\", \"gepeng\": \"gelandangan dan pengemis\", \"ghiy\": \"lagi\", \"gile\": \"gila\", \"gimana\": \"bagaimana\", \"gino\": \"gigi nongol\", \"githu\": \"gitu\", \"gj\": \"tidak jelas\", \"gmana\": \"bagaimana\", \"gn\": \"begini\", \"goblok\": \"bodoh\", \"golput\": \"golongan putih\", \"gowes\": \"mengayuh sepeda\", \"gpny\": \"tidak punya\", \"gr\": \"gede rasa\", \"gretongan\": \"gratisan\", \"gtau\": \"tidak tahu\", \"gua\": \"saya\", \"guoblok\": \"goblok\", \"gw\": \"saya\", \"ha\": \"tertawa\", \"haha\": \"tertawa\", \"hallow\": \"halo\", \"hankam\": \"pertahanan dan keamanan\", \"hehe\": \"he\", \"helo\": \"halo\", \"hey\": \"hai\", \"hlm\": \"halaman\", \"hny\": \"hanya\", \"hoax\": \"isu bohong\", \"hr\": \"hari\", \"hrus\": \"harus\", \"hubdar\": \"perhubungan darat\", \"huff\": \"mengeluh\", \"hum\": \"rumah\", \"humz\": \"rumah\", \"ilang\": \"hilang\", \"ilfil\": \"tidak suka\", \"imho\": \"in my humble opinion\", \"imoetz\": \"imut\", \"item\": \"hitam\", \"itungan\": \"hitungan\", \"iye\": \"iya\", \"ja\": \"saja\", \"jadiin\": \"jadi\", \"jaim\": \"jaga image\", \"jayus\": \"tidak lucu\", \"jdi\": \"jadi\", \"jem\": \"jam\", \"jga\": \"juga\", \"jgnkan\": \"jangankan\", \"jir\": \"anjing\", \"jln\": \"jalan\", \"jomblo\": \"tidak punya pacar\", \"jubir\": \"juru bicara\", \"jutek\": \"galak\", \"k\": \"ke\", \"kab\": \"kabupaten\", \"kabor\": \"kabur\", \"kacrut\": \"kacau\", \"kadiv\": \"kepala divisi\", \"kagak\": \"tidak\", \"kalo\": \"kalau\", \"kampret\": \"sialan\", \"kamtibmas\": \"keamanan dan ketertiban masyarakat\", \"kamuwh\": \"kamu\", \"kanwil\": \"kantor wilayah\", \"karna\": \"karena\", \"kasubbag\": \"kepala subbagian\", \"katrok\": \"kampungan\", \"kayanya\": \"kayaknya\", \"kbr\": \"kabar\", \"kdu\": \"harus\", \"kec\": \"kecamatan\", \"kejurnas\": \"kejuaraan nasional\", \"kekeuh\": \"keras kepala\", \"kel\": \"kelurahan\", \"kemaren\": \"kemarin\", \"kepengen\": \"mau\", \"kepingin\": \"mau\", \"kepsek\": \"kepala sekolah\", \"kesbang\": \"kesatuan bangsa\", \"kesra\": \"kesejahteraan rakyat\", \"ketrima\": \"diterima\", \"kgiatan\": \"kegiatan\", \"kibul\": \"bohong\", \"kimpoi\": \"kawin\", \"kl\": \"kalau\", \"klianz\": \"kalian\", \"kloter\": \"kelompok terbang\", \"klw\": \"kalau\", \"km\": \"kamu\", \"kmps\": \"kampus\", \"kmrn\": \"kemarin\", \"knal\": \"kenal\", \"knp\": \"kenapa\", \"kodya\": \"kota madya\", \"komdis\": \"komisi disiplin\", \"komsov\": \"komunis sovyet\", \"kongkow\": \"kumpul bareng teman-teman\", \"kopdar\": \"kopi darat\", \"korup\": \"korupsi\", \"kpn\": \"kapan\", \"krenz\": \"keren\", \"krm\": \"kirim\", \"kt\": \"kita\", \"ktmu\": \"ketemu\", \"ktr\": \"kantor\", \"kuper\": \"kurang pergaulan\", \"kw\": \"imitasi\", \"kyk\": \"seperti\", \"la\": \"lah\", \"lam\": \"salam\", \"lamp\": \"lampiran\", \"lanud\": \"landasan udara\", \"latgab\": \"latihan gabungan\", \"lebay\": \"berlebihan\", \"leh\": \"boleh\", \"lelet\": \"lambat\", \"lemot\": \"lambat\", \"lgi\": \"lagi\", \"lgsg\": \"langsung\", \"liat\": \"lihat\", \"litbang\": \"penelitian dan pengembangan\", \"lmyn\": \"lumayan\", \"lo\": \"kamu\", \"loe\": \"kamu\", \"lola\": \"lambat berfikir\", \"louph\": \"cinta\", \"low\": \"kalau\", \"lp\": \"lupa\", \"luber\": \"langsung, umum, bebas, dan rahasia\", \"luchuw\": \"lucu\", \"lum\": \"belum\", \"luthu\": \"lucu\", \"lwn\": \"lawan\", \"maacih\": \"terima kasih\", \"mabal\": \"bolos\", \"macem\": \"macam\", \"macih\": \"masih\", \"maem\": \"makan\", \"magabut\": \"makan gaji buta\", \"maho\": \"homo\", \"mak jang\": \"kaget\", \"maksain\": \"memaksa\", \"malem\": \"malam\", \"mam\": \"makan\", \"maneh\": \"kamu\", \"maniez\": \"manis\", \"mao\": \"mau\", \"masukin\": \"masukkan\", \"melu\": \"ikut\", \"mepet\": \"dekat sekali\", \"mgu\": \"minggu\", \"migas\": \"minyak dan gas bumi\", \"mikol\": \"minuman beralkohol\", \"miras\": \"minuman keras\", \"mlah\": \"malah\", \"mngkn\": \"mungkin\", \"mo\": \"mau\", \"mokad\": \"mati\", \"moso\": \"masa\", \"mpe\": \"sampai\", \"msk\": \"masuk\", \"mslh\": \"masalah\", \"mt\": \"makan teman\", \"mubes\": \"musyawarah besar\", \"mulu\": \"melulu\", \"mumpung\": \"selagi\", \"munas\": \"musyawarah nasional\", \"muntaber\": \"muntah dan berak\", \"musti\": \"mesti\", \"muupz\": \"maaf\", \"mw\": \"now watching\", \"n\": \"dan\", \"nanam\": \"menanam\", \"nanya\": \"bertanya\", \"napa\": \"kenapa\", \"napi\": \"narapidana\", \"napza\": \"narkotika, alkohol, psikotropika, dan zat adiktif \", \"narkoba\": \"narkotika, psikotropika, dan obat terlarang\", \"nasgor\": \"nasi goreng\", \"nda\": \"tidak\", \"ndiri\": \"sendiri\", \"ne\": \"ini\", \"nekolin\": \"neokolonialisme\", \"nembak\": \"menyatakan cinta\", \"ngabuburit\": \"menunggu berbuka puasa\", \"ngaku\": \"mengaku\", \"ngambil\": \"mengambil\", \"nganggur\": \"tidak punya pekerjaan\", \"ngapah\": \"kenapa\", \"ngaret\": \"terlambat\", \"ngasih\": \"memberikan\", \"ngebandel\": \"berbuat bandel\", \"ngegosip\": \"bergosip\", \"ngeklaim\": \"mengklaim\", \"ngeksis\": \"menjadi eksis\", \"ngeles\": \"berkilah\", \"ngelidur\": \"menggigau\", \"ngerampok\": \"merampok\", \"ngga\": \"tidak\", \"ngibul\": \"berbohong\", \"ngiler\": \"mau\", \"ngiri\": \"iri\", \"ngisiin\": \"mengisikan\", \"ngmng\": \"bicara\", \"ngomong\": \"bicara\", \"ngubek2\": \"mencari-cari\", \"ngurus\": \"mengurus\", \"nie\": \"ini\", \"nih\": \"ini\", \"niyh\": \"nih\", \"nmr\": \"nomor\", \"nntn\": \"nonton\", \"nobar\": \"nonton bareng\", \"np\": \"now playing\", \"ntar\": \"nanti\", \"ntn\": \"nonton\", \"numpuk\": \"bertumpuk\", \"nutupin\": \"menutupi\", \"nyari\": \"mencari\", \"nyekar\": \"menyekar\", \"nyicil\": \"mencicil\", \"nyoblos\": \"mencoblos\", \"nyokap\": \"ibu\", \"ogah\": \"tidak mau\", \"ol\": \"online\", \"ongkir\": \"ongkos kirim\", \"oot\": \"out of topic\", \"org2\": \"orang-orang\", \"ortu\": \"orang tua\", \"otda\": \"otonomi daerah\", \"otw\": \"on the way, sedang di jalan\", \"pacal\": \"pacar\", \"pake\": \"pakai\", \"pala\": \"kepala\", \"pansus\": \"panitia khusus\", \"parpol\": \"partai politik\", \"pasutri\": \"pasangan suami istri\", \"pd\": \"pada\", \"pede\": \"percaya diri\", \"pelatnas\": \"pemusatan latihan nasional\", \"pemda\": \"pemerintah daerah\", \"pemkot\": \"pemerintah kota\", \"pemred\": \"pemimpin redaksi\", \"penjas\": \"pendidikan jasmani\", \"perda\": \"peraturan daerah\", \"perhatiin\": \"perhatikan\", \"pesenan\": \"pesanan\", \"pgang\": \"pegang\", \"pi\": \"tapi\", \"pilkada\": \"pemilihan kepala daerah\", \"pisan\": \"sangat\", \"pk\": \"penjahat kelamin\", \"plg\": \"paling\", \"pmrnth\": \"pemerintah\", \"polantas\": \"polisi lalu lintas\", \"ponpes\": \"pondok pesantren\", \"pp\": \"pulang pergi\", \"prg\": \"pergi\", \"prnh\": \"pernah\", \"psen\": \"pesan\", \"pst\": \"pasti\", \"pswt\": \"pesawat\", \"pw\": \"posisi nyaman\", \"qmu\": \"kamu\", \"rakor\": \"rapat koordinasi\", \"ranmor\": \"kendaraan bermotor\", \"re\": \"reply\", \"ref\": \"referensi\", \"rehab\": \"rehabilitasi\", \"rempong\": \"sulit\", \"repp\": \"balas\", \"restik\": \"reserse narkotika\", \"rhs\": \"rahasia\", \"rmh\": \"rumah\", \"ru\": \"baru\", \"ruko\": \"rumah toko\", \"rusunawa\": \"rumah susun sewa\", \"ruz\": \"terus\", \"saia\": \"saya\", \"salting\": \"salah tingkah\", \"sampe\": \"sampai\", \"samsek\": \"sama sekali\", \"sapose\": \"siapa\", \"satpam\": \"satuan pengamanan\", \"sbb\": \"sebagai berikut\", \"sbh\": \"sebuah\", \"sbnrny\": \"sebenarnya\", \"scr\": \"secara\", \"sdgkn\": \"sedangkan\", \"sdkt\": \"sedikit\", \"se7\": \"setuju\", \"sebelas dua belas\": \"mirip\", \"sembako\": \"sembilan bahan pokok\", \"sempet\": \"sempat\", \"sendratari\": \"seni drama tari\", \"sgt\": \"sangat\", \"shg\": \"sehingga\", \"siech\": \"sih\", \"sikon\": \"situasi dan kondisi\", \"sinetron\": \"sinema elektronik\", \"siramin\": \"siramkan\", \"sj\": \"saja\", \"skalian\": \"sekalian\", \"sklh\": \"sekolah\", \"skt\": \"sakit\", \"slesai\": \"selesai\", \"sll\": \"selalu\", \"slma\": \"selama\", \"slsai\": \"selesai\", \"smpt\": \"sempat\", \"smw\": \"semua\", \"sndiri\": \"sendiri\", \"soljum\": \"sholat jumat\", \"songong\": \"sombong\", \"sory\": \"maaf\", \"sosek\": \"sosial-ekonomi\", \"sotoy\": \"sok tahu\", \"spa\": \"siapa\", \"sppa\": \"siapa\", \"spt\": \"seperti\", \"srtfkt\": \"sertifikat\", \"stiap\": \"setiap\", \"stlh\": \"setelah\", \"suk\": \"masuk\", \"sumpek\": \"sempit\", \"syg\": \"sayang\", \"t4\": \"tempat\", \"tajir\": \"kaya\", \"tau\": \"tahu\", \"taw\": \"tahu\", \"td\": \"tadi\", \"tdk\": \"tidak\", \"teh\": \"kakak perempuan\", \"telat\": \"terlambat\", \"telmi\": \"telat berpikir\", \"temen\": \"teman\", \"tengil\": \"menyebalkan\", \"tepar\": \"terkapar\", \"tggu\": \"tunggu\", \"tgu\": \"tunggu\", \"thankz\": \"terima kasih\", \"thn\": \"tahun\", \"tilang\": \"bukti pelanggaran\", \"tipiwan\": \"TvOne\", \"tks\": \"terima kasih\", \"tlp\": \"telepon\", \"tls\": \"tulis\", \"tmbah\": \"tambah\", \"tmen2\": \"teman-teman\", \"tmpah\": \"tumpah\", \"tmpt\": \"tempat\", \"tngu\": \"tunggu\", \"tnyta\": \"ternyata\", \"tokai\": \"tai\", \"toserba\": \"toko serba ada\", \"tpi\": \"tapi\", \"trdhulu\": \"terdahulu\", \"trima\": \"terima kasih\", \"trm\": \"terima\", \"trs\": \"terus\", \"trutama\": \"terutama\", \"ts\": \"penulis\", \"tst\": \"tahu sama tahu\", \"ttg\": \"tentang\", \"tuch\": \"tuh\", \"tuir\": \"tua\", \"tw\": \"tahu\", \"u\": \"kamu\", \"ud\": \"sudah\", \"udah\": \"sudah\", \"ujg\": \"ujung\", \"ul\": \"ulangan\", \"unyu\": \"lucu\", \"uplot\": \"unggah\", \"urang\": \"saya\", \"usah\": \"perlu\", \"utk\": \"untuk\", \"valas\": \"valuta asing\", \"w/\": \"dengan\", \"wadir\": \"wakil direktur\", \"wamil\": \"wajib militer\", \"warkop\": \"warung kopi\", \"warteg\": \"warung tegal\", \"wat\": \"buat\", \"wkt\": \"waktu\", \"wtf\": \"what the fuck\", \"xixixi\": \"tertawa\", \"ya\": \"iya\", \"yap\": \"iya\", \"yaudah\": \"ya sudah\", \"yawdah\": \"ya sudah\", \"yg\": \"yang\", \"yl\": \"yang lain\", \"yo\": \"iya\", \"yowes\": \"ya sudah\", \"yup\": \"iya\", \"7an\": \"tujuan\", \"ababil\": \"abg labil\", \"acc\": \"accord\", \"adlah\": \"adalah\", \"adoh\": \"aduh\", \"aha\": \"tertawa\", \"aing\": \"saya\", \"aja\": \"saja\", \"ajj\": \"saja\", \"aka\": \"dikenal juga sebagai\", \"akko\": \"aku\", \"akku\": \"aku\", \"akyu\": \"aku\", \"aljasa\": \"asal jadi saja\", \"ama\": \"sama\", \"ambl\": \"ambil\", \"anjir\": \"anjing\", \"ank\": \"anak\", \"ap\": \"apa\", \"apaan\": \"apa\", \"ape\": \"apa\", \"aplot\": \"unggah\", \"apva\": \"apa\", \"aqu\": \"aku\", \"asap\": \"sesegera mungkin\", \"aseek\": \"asyik\", \"asek\": \"asyik\", \"aseknya\": \"asyiknya\", \"asoy\": \"asyik\", \"astrojim\": \"astagfirullahaladzim\", \"ath\": \"kalau begitu\", \"atuh\": \"kalau begitu\", \"ava\": \"avatar\", \"aws\": \"awas\", \"ayang\": \"sayang\", \"ayok\": \"ayo\", \"bacot\": \"banyak bicara\", \"bales\": \"balas\", \"bangdes\": \"pembangunan desa\", \"bangkotan\": \"tua\", \"banpres\": \"bantuan presiden\", \"bansarkas\": \"bantuan sarana kesehatan\", \"bazis\": \"badan amal, zakat, infak, dan sedekah\", \"bcoz\": \"karena\", \"beb\": \"sayang\", \"bejibun\": \"banyak\", \"belom\": \"belum\", \"bener\": \"benar\", \"ber2\": \"berdua\", \"berdikari\": \"berdiri di atas kaki sendiri\", \"bet\": \"banget\", \"beti\": \"beda tipis\", \"beut\": \"banget\", \"bgd\": \"banget\", \"bgs\": \"bagus\", \"bhubu\": \"tidur\", \"bimbuluh\": \"bimbingan dan penyuluhan\", \"bisi\": \"kalau-kalau\", \"bkn\": \"bukan\", \"bl\": \"beli\", \"blg\": \"bilang\", \"blm\": \"belum\", \"bls\": \"balas\", \"bnchi\": \"benci\", \"bngung\": \"bingung\", \"bnyk\": \"banyak\", \"bohay\": \"badan aduhai\", \"bokep\": \"porno\", \"bokin\": \"pacar\", \"bole\": \"boleh\", \"bolot\": \"bodoh\", \"bonyok\": \"ayah ibu\", \"bpk\": \"bapak\", \"brb\": \"segera kembali\", \"brngkt\": \"berangkat\", \"brp\": \"berapa\", \"brur\": \"saudara laki-laki\", \"bsa\": \"bisa\", \"bsk\": \"besok\", \"bu_bu\": \"tidur\", \"bubarin\": \"bubarkan\", \"buber\": \"buka bersama\", \"bujubune\": \"luar biasa\", \"buser\": \"buru sergap\", \"bwhn\": \"bawahan\", \"byar\": \"bayar\", \"byr\": \"bayar\", \"c8\": \"chat\", \"cabut\": \"pergi\", \"caem\": \"cakep\", \"cama-cama\": \"sama-sama\", \"cangcut\": \"celana dalam\", \"cape\": \"capek\", \"caur\": \"jelek\", \"cekak\": \"tidak ada uang\", \"cekidot\": \"coba lihat\", \"cemplungin\": \"cemplungkan\", \"ceper\": \"pendek\", \"ceu\": \"kakak perempuan\", \"cewe\": \"cewek\", \"cibuk\": \"sibuk\", \"cin\": \"cinta\", \"ciye\": \"cie\", \"ckck\": \"ck\", \"clbk\": \"cinta lama bersemi kembali\", \"cmpr\": \"campur\", \"cnenk\": \"senang\", \"congor\": \"mulut\", \"cow\": \"cowok\", \"coz\": \"karena\", \"cpa\": \"siapa\", \"gokil\": \"gila\", \"gombal\": \"suka merayu\", \"gpl\": \"tidak pakai lama\", \"gpp\": \"tidak apa-apa\", \"gretong\": \"gratis\", \"gt\": \"begitu\", \"gtw\": \"tidak tahu\", \"gue\": \"saya\", \"guys\": \"teman-teman\", \"gws\": \"cepat sembuh\", \"haghaghag\": \"tertawa\", \"hakhak\": \"tertawa\", \"handak\": \"bahan peledak\", \"hansip\": \"pertahanan sipil\", \"hellow\": \"halo\", \"helow\": \"halo\", \"hi\": \"hai\", \"hlng\": \"hilang\", \"hnya\": \"hanya\", \"houm\": \"rumah\", \"hrs\": \"harus\", \"hubad\": \"hubungan angkatan darat\", \"hubla\": \"perhubungan laut\", \"huft\": \"mengeluh\", \"humas\": \"hubungan masyarakat\", \"idk\": \"saya tidak tahu\", \"ilfeel\": \"tidak suka\", \"imba\": \"jago sekali\", \"imoet\": \"imut\", \"info\": \"informasi\", \"itung\": \"hitung\", \"isengin\": \"bercanda\", \"iyala\": \"iya lah\", \"iyo\": \"iya\", \"jablay\": \"jarang dibelai\", \"jadul\": \"jaman dulu\", \"jancuk\": \"anjing\", \"jd\": \"jadi\", \"jdikan\": \"jadikan\", \"jg\": \"juga\", \"jgn\": \"jangan\", \"jijay\": \"jijik\", \"jkt\": \"jakarta\", \"jnj\": \"janji\", \"jth\": \"jatuh\", \"jurdil\": \"jujur adil\", \"jwb\": \"jawab\", \"ka\": \"kakak\", \"kabag\": \"kepala bagian\", \"kacian\": \"kasihan\", \"kadit\": \"kepala direktorat\", \"kaga\": \"tidak\", \"kaka\": \"kakak\", \"kamtib\": \"keamanan dan ketertiban\", \"kamuh\": \"kamu\", \"kamyu\": \"kamu\", \"kapt\": \"kapten\", \"kasat\": \"kepala satuan\", \"kasubbid\": \"kepala subbidang\", \"kau\": \"kamu\", \"kbar\": \"kabar\", \"kcian\": \"kasihan\", \"keburu\": \"terlanjur\", \"kedubes\": \"kedutaan besar\", \"kek\": \"seperti\", \"keknya\": \"kayaknya\", \"keliatan\": \"kelihatan\", \"keneh\": \"masih\", \"kepikiran\": \"terpikirkan\", \"kepo\": \"mau tahu urusan orang\", \"kere\": \"tidak punya uang\", \"kesian\": \"kasihan\", \"ketauan\": \"ketahuan\", \"keukeuh\": \"keras kepala\", \"khan\": \"kan\", \"kibus\": \"kaki busuk\", \"kk\": \"kakak\", \"klian\": \"kalian\", \"klo\": \"kalau\", \"kluarga\": \"keluarga\", \"klwrga\": \"keluarga\", \"kmari\": \"kemari\", \"kmpus\": \"kampus\", \"kn\": \"kan\", \"knl\": \"kenal\", \"knpa\": \"kenapa\", \"kog\": \"kok\", \"kompi\": \"komputer\", \"komtiong\": \"komunis Tiongkok\", \"konjen\": \"konsulat jenderal\", \"koq\": \"kok\", \"kpd\": \"kepada\", \"kptsan\": \"keputusan\", \"krik\": \"garing\", \"krn\": \"karena\", \"ktauan\": \"ketahuan\", \"ktny\": \"katanya\", \"kudu\": \"harus\", \"kuq\": \"kok\", \"ky\": \"seperti\", \"kykny\": \"kayanya\", \"laka\": \"kecelakaan\", \"lambreta\": \"lambat\", \"lansia\": \"lanjut usia\", \"lapas\": \"lembaga pemasyarakatan\", \"lbur\": \"libur\", \"lekong\": \"laki-laki\", \"lg\": \"lagi\", \"lgkp\": \"lengkap\", \"lht\": \"lihat\", \"linmas\": \"perlindungan masyarakat\", \"lmyan\": \"lumayan\", \"lngkp\": \"lengkap\", \"loch\": \"loh\", \"lol\": \"tertawa\", \"lom\": \"belum\", \"loupz\": \"cinta\", \"lowh\": \"kamu\", \"lu\": \"kamu\", \"luchu\": \"lucu\", \"luff\": \"cinta\", \"luph\": \"cinta\", \"lw\": \"kamu\", \"lwt\": \"lewat\", \"maaciw\": \"terima kasih\", \"mabes\": \"markas besar\", \"macem-macem\": \"macam-macam\", \"madesu\": \"masa depan suram\", \"maen\": \"main\", \"mahatma\": \"maju sehat bersama\", \"mak\": \"ibu\", \"makasih\": \"terima kasih\", \"malah\": \"bahkan\", \"malu2in\": \"memalukan\", \"mamz\": \"makan\", \"manies\": \"manis\", \"mantep\": \"mantap\", \"markus\": \"makelar kasus\", \"mba\": \"mbak\", \"mending\": \"lebih baik\", \"mgkn\": \"mungkin\", \"mhn\": \"mohon\", \"miker\": \"minuman keras\", \"milis\": \"mailing list\", \"mksd\": \"maksud\", \"mls\": \"malas\", \"mnt\": \"minta\", \"moge\": \"motor gede\", \"mokat\": \"mati\", \"mosok\": \"masa\", \"msh\": \"masih\", \"mskpn\": \"meskipun\", \"msng2\": \"masing-masing\", \"muahal\": \"mahal\", \"muker\": \"musyawarah kerja\", \"mumet\": \"pusing\", \"muna\": \"munafik\", \"munaslub\": \"musyawarah nasional luar biasa\", \"musda\": \"musyawarah daerah\", \"muup\": \"maaf\", \"muuv\": \"maaf\", \"nal\": \"kenal\", \"nangis\": \"menangis\", \"naon\": \"apa\", \"napol\": \"narapidana politik\", \"naq\": \"anak\", \"narsis\": \"bangga pada diri sendiri\", \"nax\": \"anak\", \"ndak\": \"tidak\", \"ndut\": \"gendut\", \"nekolim\": \"neokolonialisme\", \"nelfon\": \"menelepon\", \"ngabis2in\": \"menghabiskan\", \"ngakak\": \"tertawa\", \"ngambek\": \"marah\", \"ngampus\": \"pergi ke kampus\", \"ngantri\": \"mengantri\", \"ngapain\": \"sedang apa\", \"ngaruh\": \"berpengaruh\", \"ngawur\": \"berbicara sembarangan\", \"ngeceng\": \"kumpul bareng-bareng\", \"ngeh\": \"sadar\", \"ngekos\": \"tinggal di kos\", \"ngelamar\": \"melamar\", \"ngeliat\": \"melihat\", \"ngemeng\": \"bicara terus-terusan\", \"ngerti\": \"mengerti\", \"nggak\": \"tidak\", \"ngikut\": \"ikut\", \"nginep\": \"menginap\", \"ngisi\": \"mengisi\", \"ngmg\": \"bicara\", \"ngocol\": \"lucu\", \"ngomongin\": \"membicarakan\", \"ngumpul\": \"berkumpul\", \"ni\": \"ini\", \"nyasar\": \"tersesat\", \"nyariin\": \"mencari\", \"nyiapin\": \"mempersiapkan\", \"nyiram\": \"menyiram\", \"nyok\": \"ayo\", \"o/\": \"oleh\", \"ok\": \"ok\", \"priksa\": \"periksa\", \"pro\": \"profesional\", \"psn\": \"pesan\", \"psti\": \"pasti\", \"puanas\": \"panas\", \"qmo\": \"kamu\", \"qt\": \"kita\", \"rame\": \"ramai\", \"raskin\": \"rakyat miskin\", \"red\": \"redaksi\", \"reg\": \"register\", \"rejeki\": \"rezeki\", \"renstra\": \"rencana strategis\", \"reskrim\": \"reserse kriminal\", \"sni\": \"sini\", \"somse\": \"sombong sekali\", \"sorry\": \"maaf\", \"sosbud\": \"sosial-budaya\", \"sospol\": \"sosial-politik\", \"sowry\": \"maaf\", \"spd\": \"sepeda\", \"sprti\": \"seperti\", \"spy\": \"supaya\", \"stelah\": \"setelah\", \"subbag\": \"subbagian\", \"sumbangin\": \"sumbangkan\", \"sy\": \"saya\", \"syp\": \"siapa\", \"tabanas\": \"tabungan pembangunan nasional\", \"tar\": \"nanti\", \"taun\": \"tahun\", \"tawh\": \"tahu\", \"tdi\": \"tadi\", \"te2p\": \"tetap\", \"tekor\": \"rugi\", \"telkom\": \"telekomunikasi\", \"telp\": \"telepon\", \"temen2\": \"teman-teman\", \"tengok\": \"menjenguk\", \"terbitin\": \"terbitkan\", \"tgl\": \"tanggal\", \"thanks\": \"terima kasih\", \"thd\": \"terhadap\", \"thx\": \"terima kasih\", \"tipi\": \"TV\", \"tkg\": \"tukang\", \"tll\": \"terlalu\", \"tlpn\": \"telepon\", \"tman\": \"teman\", \"tmbh\": \"tambah\", \"tmn2\": \"teman-teman\", \"tmph\": \"tumpah\", \"tnda\": \"tanda\", \"tnh\": \"tanah\", \"togel\": \"toto gelap\", \"tp\": \"tapi\", \"tq\": \"terima kasih\", \"trgntg\": \"tergantung\", \"trims\": \"terima kasih\", \"cb\": \"coba\", \"y\": \"ya\", \"munfik\": \"munafik\", \"reklamuk\": \"reklamasi\", \"sma\": \"sama\", \"tren\": \"trend\", \"ngehe\": \"kesal\", \"mz\": \"mas\", \"analisise\": \"analisis\", \"sadaar\": \"sadar\", \"sept\": \"september\", \"nmenarik\": \"menarik\", \"zonk\": \"bodoh\", \"rights\": \"benar\", \"simiskin\": \"miskin\", \"ngumpet\": \"sembunyi\", \"hardcore\": \"keras\", \"akhirx\": \"akhirnya\", \"solve\": \"solusi\", \"watuk\": \"batuk\", \"ngebully\": \"intimidasi\", \"masy\": \"masyarakat\", \"still\": \"masih\", \"tauk\": \"tahu\", \"mbual\": \"bual\", \"tioghoa\": \"tionghoa\", \"ngentotin\": \"senggama\", \"kentot\": \"senggama\", \"faktakta\": \"fakta\", \"sohib\": \"teman\", \"rubahnn\": \"rubah\", \"trlalu\": \"terlalu\", \"nyela\": \"cela\", \"heters\": \"pembenci\", \"nyembah\": \"sembah\", \"most\": \"paling\", \"ikon\": \"lambang\", \"light\": \"terang\", \"pndukung\": \"pendukung\", \"setting\": \"atur\", \"seting\": \"akting\", \"next\": \"lanjut\", \"waspadalah\": \"waspada\", \"gantengsaya\": \"ganteng\", \"parte\": \"partai\", \"nyerang\": \"serang\", \"nipu\": \"tipu\", \"ktipu\": \"tipu\", \"jentelmen\": \"berani\", \"buangbuang\": \"buang\", \"tsangka\": \"tersangka\", \"kurng\": \"kurang\", \"ista\": \"nista\", \"less\": \"kurang\", \"koar\": \"teriak\", \"paranoid\": \"takut\", \"problem\": \"masalah\", \"tahi\": \"kotoran\", \"tirani\": \"tiran\", \"tilep\": \"tilap\", \"happy\": \"bahagia\", \"tak\": \"tidak\", \"penertiban\": \"tertib\", \"uasai\": \"kuasa\", \"mnolak\": \"tolak\", \"trending\": \"trend\", \"taik\": \"tahi\", \"wkwkkw\": \"tertawa\", \"ahokncc\": \"ahok\", \"istaa\": \"nista\", \"benarjujur\": \"jujur\", \"mgkin\": \"mungkin\"}\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text\n",
    "\n",
    "def tokenizing_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def filtering_text(tokens):\n",
    "    stopwords_ind = set(stopwords.words('indonesian'))\n",
    "    stopwords_eng = set(stopwords.words('english'))\n",
    "\n",
    "    custom_stopwords = {\n",
    "        'iya', 'yaa', 'gak', 'nya', 'na', 'sih', 'ku', 'di',\n",
    "        'ga', 'ya', 'gaa', 'loh', 'kah', 'woi', 'woii', 'woy'\n",
    "    }\n",
    "\n",
    "    combined_stopwords = stopwords_ind.union(stopwords_eng).union(custom_stopwords)\n",
    "\n",
    "    filtered_tokens = [word for word in tokens if word not in combined_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "def stemming_text(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def to_sentence(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c51a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text cleaning functions to the DataFrame\n",
    "def preprocess_text(df):\n",
    "    df['cleaning_text'] = df['Review'].apply(cleaning_text)\n",
    "    df['casefolding_text'] = df['cleaning_text'].apply(casefolding_text)\n",
    "    df['fix_slangwords'] = df['casefolding_text'].apply(fix_slangwords)\n",
    "    df['tokenizing_text'] = df['fix_slangwords'].apply(tokenizing_text)\n",
    "    df['filtering_text'] = df['tokenizing_text'].apply(filtering_text)\n",
    "    df['clean_text'] = df['filtering_text'].apply(to_sentence)\n",
    "    return df\n",
    "\n",
    "clean_df = preprocess_text(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77817e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58439 entries, 0 to 58499\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Review            58439 non-null  object\n",
      " 1   cleaning_text     58439 non-null  object\n",
      " 2   casefolding_text  58439 non-null  object\n",
      " 3   fix_slangwords    58439 non-null  object\n",
      " 4   tokenizing_text   58439 non-null  object\n",
      " 5   filtering_text    58439 non-null  object\n",
      " 6   clean_text        58439 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Show summary information of the cleaned DataFrame\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54be03b",
   "metadata": {},
   "source": [
    "# **Labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3946ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentiment dictionary data\n",
    "def load_lexicon(url):\n",
    "    \"\"\"\n",
    "    Fetch CSV data from URL and return dictionary of word:score.\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "        for row in reader:\n",
    "            lexicon[row[0]] = int(row[1])\n",
    "    else:\n",
    "        print(f\"Failed to fetch lexicon data from {url}\")\n",
    "    return lexicon\n",
    "\n",
    "lexicon_positive = load_lexicon('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "\n",
    "lexicon_negative = load_lexicon('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de6f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment score and polarity from word list\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment score and polarity for a list of words using positive and negative lexicons.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "\n",
    "    for word in text:\n",
    "        if word in lexicon_positive:\n",
    "            score += lexicon_positive[word]\n",
    "        if word in lexicon_negative:\n",
    "            score += lexicon_negative[word]\n",
    "\n",
    "    if score > 0:\n",
    "        polarity = 'positif'\n",
    "    elif score < 0:\n",
    "        polarity = 'negatif'\n",
    "    else:\n",
    "        polarity = 'netral'\n",
    "\n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70f414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "negatif    33556\n",
      "positif    18669\n",
      "netral      6214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply sentiment analysis and add results to new columns\n",
    "results = clean_df['filtering_text'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "\n",
    "results = list(zip(*results))\n",
    "\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "\n",
    "print(clean_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af8635",
   "metadata": {},
   "source": [
    "# **Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0474576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and TF-IDF feature extraction for Machine Learning\n",
    "clean_df['label'] = clean_df['polarity'].astype('category').cat.codes\n",
    "y_ml = clean_df['label'].values\n",
    "X_ml = clean_df['clean_text'].values\n",
    "\n",
    "X_ml_train, X_ml_test, y_ml_train, y_ml_test = train_test_split(\n",
    "    X_ml, y_ml, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_ml_train_tfidf = tfidf.fit_transform(X_ml_train)\n",
    "X_ml_test_tfidf = tfidf.transform(X_ml_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5acd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Deep Learning\n",
    "y_dl = pd.get_dummies(clean_df['polarity']).values\n",
    "X_dl = clean_df['clean_text'].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_dl)\n",
    "sequences = tokenizer.texts_to_sequences(X_dl)\n",
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "X_dl_train, X_dl_test, y_dl_train, y_dl_test = train_test_split(\n",
    "    padded_sequences, y_dl, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46c4af",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12729253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR - Training Accuracy: 0.9241\n",
      "LR - Testing Accuracy: 0.8919\n",
      "\n",
      "Classification Report - Testing Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.90      0.96      0.93     10059\n",
      "     positif       0.85      0.44      0.58      1854\n",
      "      netral       0.89      0.91      0.90      5619\n",
      "\n",
      "    accuracy                           0.89     17532\n",
      "   macro avg       0.88      0.77      0.80     17532\n",
      "weighted avg       0.89      0.89      0.88     17532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with TF-IDF for sentiment analysis\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_ml_train_tfidf, y_ml_train)\n",
    "\n",
    "y_train_pred_lr = lr_model.predict(X_ml_train_tfidf)\n",
    "y_test_pred_lr = lr_model.predict(X_ml_test_tfidf)\n",
    "\n",
    "print(f\"LR - Training Accuracy: {accuracy_score(y_ml_train, y_train_pred_lr):.4f}\")\n",
    "print(f\"LR - Testing Accuracy: {accuracy_score(y_ml_test, y_test_pred_lr):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report - Testing Set:\")\n",
    "print(classification_report(y_ml_test, y_test_pred_lr, target_names=clean_df['polarity'].astype(str).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44378d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Training Accuracy: 0.9339\n",
      "SVM - Testing Accuracy: 0.8978\n",
      "\n",
      "Classification Report - Testing Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.91      0.96      0.94     10059\n",
      "     positif       0.76      0.53      0.62      1854\n",
      "      netral       0.90      0.91      0.90      5619\n",
      "\n",
      "    accuracy                           0.90     17532\n",
      "   macro avg       0.86      0.80      0.82     17532\n",
      "weighted avg       0.89      0.90      0.89     17532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM) with TF-IDF for sentiment analysis\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_ml_train_tfidf, y_ml_train)\n",
    "\n",
    "y_train_pred_svm = svm_model.predict(X_ml_train_tfidf)\n",
    "y_test_pred_svm = svm_model.predict(X_ml_test_tfidf)\n",
    "\n",
    "print(f\"SVM - Training Accuracy: {accuracy_score(y_ml_train, y_train_pred_svm):.4f}\")\n",
    "print(f\"SVM - Testing Accuracy: {accuracy_score(y_ml_test, y_test_pred_svm):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report - Testing Set:\")\n",
    "print(classification_report(y_ml_test, y_test_pred_svm, target_names=clean_df['polarity'].astype(str).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064c3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 203ms/step - accuracy: 0.7029 - loss: 0.6867 - val_accuracy: 0.8837 - val_loss: 0.3246\n",
      "Epoch 2/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 186ms/step - accuracy: 0.9022 - loss: 0.2815 - val_accuracy: 0.9114 - val_loss: 0.2514\n",
      "Epoch 3/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 210ms/step - accuracy: 0.9249 - loss: 0.2158 - val_accuracy: 0.9167 - val_loss: 0.2288\n",
      "Epoch 4/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 206ms/step - accuracy: 0.9361 - loss: 0.1847 - val_accuracy: 0.9212 - val_loss: 0.2221\n",
      "Epoch 5/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 238ms/step - accuracy: 0.9440 - loss: 0.1577 - val_accuracy: 0.9250 - val_loss: 0.2284\n",
      "Epoch 6/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 247ms/step - accuracy: 0.9521 - loss: 0.1413 - val_accuracy: 0.9271 - val_loss: 0.2250\n",
      "Epoch 7/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 254ms/step - accuracy: 0.9558 - loss: 0.1330 - val_accuracy: 0.9281 - val_loss: 0.2476\n",
      "Epoch 8/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.9608 - loss: 0.1171\n",
      "Accuracy and val_accuracy > 93%. Training stopped.\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 249ms/step - accuracy: 0.9608 - loss: 0.1171 - val_accuracy: 0.9304 - val_loss: 0.2193\n"
     ]
    }
   ],
   "source": [
    "# LSTM model for sentiment analysis\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),\n",
    "    LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "class AccuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('accuracy') > 0.93 and logs.get('val_accuracy') > 0.93:\n",
    "            print(\"\\nAccuracy and val_accuracy > 93%. Training stopped.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "history = model.fit(\n",
    "    X_dl_train, y_dl_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_dl_test, y_dl_test),\n",
    "    callbacks=[AccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0358e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 1s/step - accuracy: 0.7026 - loss: 0.6743 - val_accuracy: 0.8909 - val_loss: 0.3051\n",
      "Epoch 2/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 1s/step - accuracy: 0.9040 - loss: 0.2739 - val_accuracy: 0.8990 - val_loss: 0.2778\n",
      "Epoch 3/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m842s\u001b[0m 1s/step - accuracy: 0.9284 - loss: 0.2072 - val_accuracy: 0.9196 - val_loss: 0.2251\n",
      "Epoch 4/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 986ms/step - accuracy: 0.9381 - loss: 0.1792 - val_accuracy: 0.9237 - val_loss: 0.2184\n",
      "Epoch 5/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938ms/step - accuracy: 0.9476 - loss: 0.1545\n",
      "Accuracy and val_accuracy > 93%. Training stopped.\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 978ms/step - accuracy: 0.9476 - loss: 0.1546 - val_accuracy: 0.9316 - val_loss: 0.2110\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM model for sentiment analysis\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_dl_train, y_dl_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_dl_test, y_dl_test),\n",
    "    callbacks=[AccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7295e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 157ms/step - accuracy: 0.7021 - loss: 0.6939 - val_accuracy: 0.8890 - val_loss: 0.3079\n",
      "Epoch 2/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 140ms/step - accuracy: 0.9018 - loss: 0.2848 - val_accuracy: 0.9066 - val_loss: 0.2543\n",
      "Epoch 3/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 145ms/step - accuracy: 0.9261 - loss: 0.2177 - val_accuracy: 0.9122 - val_loss: 0.2483\n",
      "Epoch 4/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 160ms/step - accuracy: 0.9347 - loss: 0.1879 - val_accuracy: 0.9129 - val_loss: 0.2542\n",
      "Epoch 5/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 168ms/step - accuracy: 0.9419 - loss: 0.1656 - val_accuracy: 0.9168 - val_loss: 0.2473\n",
      "Epoch 6/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 179ms/step - accuracy: 0.9502 - loss: 0.1470 - val_accuracy: 0.9291 - val_loss: 0.2163\n",
      "Epoch 7/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 175ms/step - accuracy: 0.9529 - loss: 0.1344 - val_accuracy: 0.9273 - val_loss: 0.2258\n",
      "Epoch 8/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 185ms/step - accuracy: 0.9597 - loss: 0.1200 - val_accuracy: 0.9288 - val_loss: 0.2325\n",
      "Epoch 9/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.9640 - loss: 0.1095\n",
      "Accuracy and val_accuracy > 93%. Training stopped.\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 178ms/step - accuracy: 0.9640 - loss: 0.1095 - val_accuracy: 0.9300 - val_loss: 0.2413\n"
     ]
    }
   ],
   "source": [
    "# GRU model for sentiment analysis\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),\n",
    "    GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    GRU(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_dl_train, y_dl_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_dl_test, y_dl_test),\n",
    "    callbacks=[AccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9c3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 790ms/step - accuracy: 0.7024 - loss: 0.6875 - val_accuracy: 0.8981 - val_loss: 0.2902\n",
      "Epoch 2/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 816ms/step - accuracy: 0.9095 - loss: 0.2654 - val_accuracy: 0.9134 - val_loss: 0.2377\n",
      "Epoch 3/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 814ms/step - accuracy: 0.9292 - loss: 0.2057 - val_accuracy: 0.9085 - val_loss: 0.2845\n",
      "Epoch 4/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 679ms/step - accuracy: 0.9360 - loss: 0.1841 - val_accuracy: 0.9286 - val_loss: 0.2122\n",
      "Epoch 5/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 619ms/step - accuracy: 0.9457 - loss: 0.1566 - val_accuracy: 0.9293 - val_loss: 0.2219\n",
      "Epoch 6/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 814ms/step - accuracy: 0.9505 - loss: 0.1466 - val_accuracy: 0.9288 - val_loss: 0.2191\n",
      "Epoch 7/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 938ms/step - accuracy: 0.9568 - loss: 0.1344 - val_accuracy: 0.9239 - val_loss: 0.2554\n",
      "Epoch 8/50\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - accuracy: 0.9589 - loss: 0.1219\n",
      "Accuracy and val_accuracy > 93%. Training stopped.\n",
      "\u001b[1m731/731\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 1s/step - accuracy: 0.9589 - loss: 0.1219 - val_accuracy: 0.9302 - val_loss: 0.2305\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional GRU model for sentiment analysis\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_dl_train, y_dl_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_dl_test, y_dl_test),\n",
    "    callbacks=[AccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf1eaf",
   "metadata": {},
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62f0d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
      "Sentimen: negatif\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample text\n",
    "label_mapping = {0: 'negatif', 1: 'netral', 2: 'positif'}\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    cleaned = cleaning_text(text)\n",
    "    folded = casefolding_text(cleaned)\n",
    "    fixed = fix_slangwords(folded)\n",
    "    tokens = tokenizing_text(fixed)\n",
    "    filtered = filtering_text(tokens)\n",
    "    stemmed = stemming_text(' '.join(filtered))\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([stemmed])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length)\n",
    "\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_label_index = prediction.argmax(axis=1)[0]\n",
    "    sentiment = label_mapping[predicted_label_index]\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "input_text = input(\"Enter a new sentence: \")\n",
    "hasil = predict_sentiment(input_text)\n",
    "print(f\"Sentimen: {hasil}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpml1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
